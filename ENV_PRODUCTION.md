# Configuraci√≥n para Producci√≥n (Mismo Servidor)

Cuando tu aplicaci√≥n Next.js se aloja en el **mismo servidor** donde est√° Ollama, configura las variables de entorno as√≠:

## Configuraci√≥n Recomendada

En tu archivo `.env` o `.env.production`:

```env
# API Directa de Ollama (mismo servidor)
OLLAMA_DIRECT_URL=http://localhost:11434

# NO configurar estas variables si usas API directa:
# OPEN_WEBUI_URL=
# OPEN_WEBUI_API_KEY=
```

## Ventajas de Usar API Directa

‚úÖ **Modelfile funciona correctamente**: Los modelos personalizados respetan el SYSTEM prompt del Modelfile sin necesidad de enviarlo en cada petici√≥n

‚úÖ **Mejor rendimiento**: Sin overhead de proxy ni autenticaci√≥n

‚úÖ **Menos tokens**: El system prompt ya est√° incorporado en el modelo, solo se env√≠a el historial de conversaci√≥n

‚úÖ **M√°s confiable**: Conexi√≥n directa sin intermediarios

## C√≥mo Funciona

1. **Detecci√≥n autom√°tica**: Si `OLLAMA_DIRECT_URL` est√° configurado o detecta `localhost:11434`, usa API directa
2. **Modelos personalizados**: Cuando creas un bot con `system_prompt`, se crea un modelo personalizado usando Modelfile
3. **Uso del Modelfile**: Si usas modelo personalizado en API directa, **solo se env√≠a el historial** (sin system prompt), confiando en el Modelfile
4. **Modelos normales**: Si usas un modelo base, siempre se env√≠a el system prompt en cada petici√≥n

## Logs Esperados

Cuando uses API directa con modelo personalizado, ver√°s:

```
üîß OllamaClient Configuration:
  Base URL: http://localhost:11434
  Using Direct Ollama: ‚úÖ YES (Same Server)
  Using Open WebUI: ‚ùå NO

üí¨ Chat Request:
  Custom Model: ‚úÖ YES (Modelfile only)
  Using Direct Ollama: ‚úÖ YES (Same Server)
  API: ü¶ô Direct Ollama
  Using Modelfile only: ‚úÖ YES

ü¶ô Using Direct Ollama API (Custom Model with Modelfile): http://localhost:11434/api/chat
üì§ Payload:
  customModel: ‚úÖ YES
  usingModelfile: ‚úÖ YES (Modelfile only)
  hasSystemPrompt: false
```

## Si Necesitas Usar Open WebUI

Si por alguna raz√≥n necesitas usar Open WebUI (otro servidor), configura:

```env
# Open WebUI en otro servidor
OPEN_WEBUI_URL=http://72.61.11.3:8080
OPEN_WEBUI_API_KEY=tu_api_key_aqui

# NO configurar OLLAMA_DIRECT_URL en este caso
```

