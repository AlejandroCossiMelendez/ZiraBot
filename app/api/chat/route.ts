import { NextResponse } from 'next/server';
import { query, queryOne } from '@/lib/db';
import { ollama } from '@/lib/ollama';
import { validateToken } from '@/lib/auth';

export async function POST(request: Request) {
  try {
    // Verificar autenticaci√≥n (Token Bearer o sesi√≥n interna)
    const authHeader = request.headers.get('Authorization');
    let companyId: number | null = null;

    if (authHeader?.startsWith('Bearer ')) {
      const token = authHeader.split(' ')[1];
      const company = await validateToken(token);
      if (!company) {
        return NextResponse.json({ error: 'Invalid token' }, { status: 401 });
      }
      companyId = company.id;
    }

    const body = await request.json();
    const { bot_id, message, session_id } = body;

    if (!bot_id || !message) {
      return NextResponse.json({ error: 'Missing required fields' }, { status: 400 });
    }

    // Obtener configuraci√≥n del bot
    const bot = await queryOne('SELECT * FROM bots WHERE id = ?', [bot_id]);
    if (!bot) {
      return NextResponse.json({ error: 'Bot not found' }, { status: 404 });
    }

    // Verificar que el bot pertenece a la empresa (si se usa token)
    if (companyId && bot.company_id !== companyId) {
      return NextResponse.json({ error: 'Unauthorized access to this bot' }, { status: 403 });
    }

    // Gestionar sesi√≥n de conversaci√≥n
    let conversationId: number;
    const currentSessionId = session_id || `sess_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    if (session_id) {
      const conversation = await queryOne(
        'SELECT id FROM conversations WHERE session_id = ? AND bot_id = ?',
        [session_id, bot_id]
      );
      
      if (conversation) {
        conversationId = conversation.id;
      } else {
        const result = await query(
          'INSERT INTO conversations (bot_id, session_id) VALUES (?, ?)',
          [bot_id, currentSessionId]
        );
        conversationId = (result as any).insertId;
      }
    } else {
      const result = await query(
        'INSERT INTO conversations (bot_id, session_id) VALUES (?, ?)',
        [bot_id, currentSessionId]
      );
      conversationId = (result as any).insertId;
    }

    // Guardar mensaje del usuario
    await query(
      "INSERT INTO messages (conversation_id, role, content) VALUES (?, 'user', ?)",
      [conversationId, message]
    );

    // Obtener historial reciente para contexto (√∫ltimos 5 mensajes para reducir tiempo de respuesta)
    // IMPORTANTE: Ordenar por DESC para obtener los mensajes m√°s recientes
    // Reducido de 10 a 5 para acelerar el procesamiento
    const history = await query(
      "SELECT role, content FROM messages WHERE conversation_id = ? ORDER BY created_at DESC LIMIT 5",
      [conversationId]
    );
    
    // Revertir el orden para mantener la secuencia cronol√≥gica correcta (m√°s antiguo primero)
    history.reverse();

    // Verificar si el bot usa un modelo personalizado (con system prompt incorporado)
    const usingCustomModel = !!bot.custom_model_name;
    const modelToUse = bot.custom_model_name || bot.model;
    
    // Filtrar el historial para excluir mensajes system previos y mantener solo user/assistant
    const conversationHistory = history
      .filter((msg: any) => msg.role !== 'system')
      .map((msg: any) => ({ role: msg.role, content: msg.content }));
    
    // Preparar system prompt del bot con instrucciones MUY estrictas
    const baseSystemPrompt = bot.system_prompt || 'You are a helpful assistant.';
    
    // Extraer el tema principal del conocimiento para usar en las respuestas
    const knowledgeTopic = baseSystemPrompt.includes('tecnolog√≠a') || baseSystemPrompt.includes('desarrollo') || baseSystemPrompt.includes('software')
      ? 'tecnolog√≠a, desarrollo de software y arquitectura de sistemas'
      : 'el tema del conocimiento proporcionado';
    
    const systemPrompt = bot.system_prompt 
      ? `‚ö†Ô∏è ADVERTENCIA CR√çTICA: ESTAS INSTRUCCIONES SON ABSOLUTAS Y NO NEGOCIABLES ‚ö†Ô∏è

REGLA FUNDAMENTAL #1: SOLO puedes usar la informaci√≥n del "CONOCIMIENTO PROPORCIONADO" que aparece m√°s abajo.
REGLA FUNDAMENTAL #2: NUNCA uses tu conocimiento general, entrenamiento previo, o cualquier informaci√≥n que no est√© expl√≠citamente en el "CONOCIMIENTO PROPORCIONADO".

INSTRUCCIONES ABSOLUTAS:
1. Si te preguntan algo que NO est√° en el "CONOCIMIENTO PROPORCIONADO", DEBES responder EXACTAMENTE: "No tengo informaci√≥n sobre eso en mi conocimiento proporcionado. Solo puedo responder preguntas relacionadas con ${knowledgeTopic}."

2. NUNCA respondas preguntas sobre:
   - Matem√°ticas b√°sicas (a menos que est√© en el conocimiento proporcionado)
   - Historia, filosof√≠a, acertijos, chistes
   - Cualquier tema que NO est√© mencionado en el "CONOCIMIENTO PROPORCIONADO"
   - Informaci√≥n general que aprendiste durante tu entrenamiento

3. Si la pregunta est√° relacionada con el conocimiento proporcionado, responde SOLO con esa informaci√≥n.

4. Si la pregunta NO est√° relacionada, responde: "No tengo informaci√≥n sobre eso en mi conocimiento proporcionado."

EJEMPLOS DE RESPUESTAS CORRECTAS:
- Pregunta: "¬øCu√°nto es 5x5?" ‚Üí Respuesta: "No tengo informaci√≥n sobre operaciones matem√°ticas b√°sicas en mi conocimiento proporcionado. Solo puedo responder preguntas relacionadas con ${knowledgeTopic}."
- Pregunta: "¬øQu√© fue primero el huevo o la gallina?" ‚Üí Respuesta: "No tengo informaci√≥n sobre ese tema en mi conocimiento proporcionado. Solo puedo responder preguntas relacionadas con ${knowledgeTopic}."

CONOCIMIENTO PROPORCIONADO (√öNICA FUENTE DE INFORMACI√ìN PERMITIDA):
${baseSystemPrompt}

‚ö†Ô∏è RECORDATORIO FINAL: Si la pregunta NO est√° relacionada con el "CONOCIMIENTO PROPORCIONADO" de arriba, responde que no tienes informaci√≥n. NUNCA uses conocimiento general.`
      : 'You are a helpful assistant. Only respond based on the information explicitly provided to you. If you do not have information about something, clearly state "I do not have information about that" or "I do not know".';
    
    // IMPORTANTE: Siempre enviar el system prompt expl√≠citamente
    // Aunque ahora usamos el comando `ollama create` directamente (que aplica correctamente el SYSTEM),
    // enviamos el system prompt expl√≠citamente en cada petici√≥n como respaldo para garantizar que siempre se aplique
    const config = ollama.getConfig();
    
    // Siempre incluir el system prompt en los mensajes
    const ollamaMessages = [
      { role: 'system', content: systemPrompt },
      ...conversationHistory
    ];

    // Log de configuraci√≥n (siempre en desarrollo, o si ENABLE_LOGS est√° habilitado)
    const shouldLog = process.env.NODE_ENV !== 'production' || process.env.ENABLE_LOGS === 'true';
    if (shouldLog) {
      console.log('üí¨ Chat Request:');
      console.log('  Bot ID:', bot_id);
      console.log('  Model:', modelToUse);
      console.log('  Custom Model:', usingCustomModel ? '‚úÖ YES (using explicit system prompt)' : '‚ùå NO');
      console.log('  Using Direct Ollama:', config.useDirectOllama ? '‚úÖ YES (Same Server)' : '‚ùå NO');
      console.log('  API:', config.useDirectOllama ? 'ü¶ô Direct Ollama' : (config.isOpenWebUI ? 'üåê Open WebUI (Cloud)' : 'ü¶ô Local Ollama'));
      console.log('  Base URL:', config.baseUrl);
      console.log('  Messages count:', ollamaMessages.length);
      console.log('  History messages:', conversationHistory.length);
      console.log('  System prompt (first 200 chars):', systemPrompt.substring(0, 200));
    }

    // Generar respuesta con Ollama
    // Convertir valores num√©ricos a n√∫meros (pueden venir como strings desde la DB)
    // Usar temperatura m√°s baja por defecto (0.3) para respuestas m√°s deterministas y controladas
    const temperature = typeof bot.temperature === 'string' 
      ? parseFloat(bot.temperature) 
      : (bot.temperature ?? 0.3);
    
    // Calcular max_tokens de forma inteligente
    // Si el usuario configur√≥ un valor muy bajo (< 500), aumentarlo autom√°ticamente para evitar respuestas cortadas
    // Esto balancea velocidad con completitud de respuestas
    let maxTokens = typeof bot.max_tokens === 'string'
      ? parseInt(bot.max_tokens, 10)
      : (bot.max_tokens ?? 500);
    
    // Si max_tokens es muy bajo, aumentarlo autom√°ticamente para evitar respuestas cortadas
    // 500 es un buen balance entre velocidad y completitud
    if (maxTokens < 500) {
      if (shouldLog) {
        console.log(`‚ö†Ô∏è max_tokens (${maxTokens}) es muy bajo, aumentando a 500 para evitar respuestas cortadas`);
      }
      maxTokens = 500;
    }

    // Siempre enviar el system prompt expl√≠citamente (no confiar en Modelfile)
    // skipSystemPrompt=false para que siempre se incluya el system prompt en los mensajes
    const response = await ollama.generate({
      model: modelToUse,
      messages: ollamaMessages as any,
      options: {
        temperature: temperature,
        num_predict: maxTokens
      }
    }, false); // Siempre false - siempre enviar system prompt expl√≠citamente

    const assistantMessage = response.message?.content || '';

    // Guardar respuesta del asistente
    await query(
      "INSERT INTO messages (conversation_id, role, content) VALUES (?, 'assistant', ?)",
      [conversationId, assistantMessage]
    );

    return NextResponse.json({
      response: assistantMessage,
      session_id: currentSessionId,
      model: bot.model
    });

  } catch (error: any) {
    console.error('Chat error:', error);
    const errorMessage = error.message || 'Error processing chat request';
    return NextResponse.json({ 
      error: errorMessage.includes('timeout') || errorMessage.includes('tard√≥ demasiado')
        ? 'La solicitud tard√≥ demasiado tiempo. Por favor, intenta con una pregunta m√°s corta.'
        : 'Error processing chat request'
    }, { status: 500 });
  }
}
